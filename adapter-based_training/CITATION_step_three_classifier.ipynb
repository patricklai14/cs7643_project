{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CITATION_step_three_classifier.ipynb","provenance":[],"mount_file_id":"1ujnHRxmUNM4SNXgXm5YC8TM0duOtZezG","authorship_tag":"ABX9TyOuqbjWllYIN5wA2iLnHHM9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2db29a06f2584eccaaa5fe82202bffc3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4669242fd69a491f929144a27bacb9e0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_15780094a0c54ee88445788aebe5b4a3","IPY_MODEL_1afb768a4dbf482985e4c0e87307b7b8"]}},"4669242fd69a491f929144a27bacb9e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15780094a0c54ee88445788aebe5b4a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7f5f68fc3ad940d9a6b7e2a94c072964","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9443c5cdceb49aaa6702bc1fa9154b2"}},"1afb768a4dbf482985e4c0e87307b7b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b5486e0793aa4a0a99fba1fed663d4c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:14&lt;00:00, 14.66s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_420ad8bc5ae14d30ac6628b0143697f5"}},"7f5f68fc3ad940d9a6b7e2a94c072964":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a9443c5cdceb49aaa6702bc1fa9154b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5486e0793aa4a0a99fba1fed663d4c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"420ad8bc5ae14d30ac6628b0143697f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27a06b520a9b497b9693d13aadb50029":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e6ad94fd922344f288b3bccf138f723f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d701856b2cc44c99ad9f06a7fbeb5154","IPY_MODEL_3b1d2be5957c423fb3b8d388518e8dd3"]}},"e6ad94fd922344f288b3bccf138f723f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d701856b2cc44c99ad9f06a7fbeb5154":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_42417cc2480e4548be78139b890d5761","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d336da6fe4484458a923fb39f1b561a3"}},"3b1d2be5957c423fb3b8d388518e8dd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e993e25a0338468d8e4eb4e287756e5b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  8.29ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5bf5c7403ca64e23adbee4fe01377f08"}},"42417cc2480e4548be78139b890d5761":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d336da6fe4484458a923fb39f1b561a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e993e25a0338468d8e4eb4e287756e5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5bf5c7403ca64e23adbee4fe01377f08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"022937e7bea548e49fb6addbe447eca7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b2a747c438e445fb87855eab6e1252b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fb3fed9a9ba14c1da6a92f7ce3a943e0","IPY_MODEL_0176066aed6f454d961596203d854909"]}},"b2a747c438e445fb87855eab6e1252b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb3fed9a9ba14c1da6a92f7ce3a943e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_95f3863565b6481ab1910e09f033ee7c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de41e8f9d5d04212b124667588699b0d"}},"0176066aed6f454d961596203d854909":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_180e839d0af7442bb34e03c544f80104","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:08&lt;00:00,  8.67s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ebbd8444f50e4925b74cde16065a633c"}},"95f3863565b6481ab1910e09f033ee7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"de41e8f9d5d04212b124667588699b0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"180e839d0af7442bb34e03c544f80104":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ebbd8444f50e4925b74cde16065a633c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qIS5hbagdjv","executionInfo":{"status":"ok","timestamp":1619967384496,"user_tz":240,"elapsed":18452,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"8f4ebdbe-ffe4-4083-fabe-cc0b66afd5df"},"source":["!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git\n","!pip install datasets"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/Adapter-Hub/adapter-transformers.git\n","  Cloning https://github.com/Adapter-Hub/adapter-transformers.git to /tmp/pip-req-build-20oc2574\n","  Running command git clone -q https://github.com/Adapter-Hub/adapter-transformers.git /tmp/pip-req-build-20oc2574\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (20.9)\n","Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (1.19.5)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (0.0.45)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (2.23.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (3.0.12)\n","Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (4.41.1)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (0.10.2)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0) (3.10.1)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==2.0.0) (2.4.7)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0) (7.1.2)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0) (1.15.0)\n","Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0) (1.0.1)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0) (1.24.3)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->adapter-transformers==2.0.0) (3.4.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->adapter-transformers==2.0.0) (3.7.4.3)\n","Building wheels for collected packages: adapter-transformers\n","  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for adapter-transformers: filename=adapter_transformers-2.0.0-cp37-none-any.whl size=2097542 sha256=58e20e035696fc00c9d6bcaaeda67a3776248b9d7dc3cbaf4a38cbca0d0e86e8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5ir_tjb7/wheels/b0/56/c9/5bf1c51cd513412090ad751ab10fc025210176bf0a82dd8af3\n","Successfully built adapter-transformers\n","Installing collected packages: adapter-transformers\n","  Found existing installation: adapter-transformers 2.0.0\n","    Uninstalling adapter-transformers-2.0.0:\n","      Successfully uninstalled adapter-transformers-2.0.0\n","Successfully installed adapter-transformers-2.0.0\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.4.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHkCXJumhII7","executionInfo":{"status":"ok","timestamp":1619967401799,"user_tz":240,"elapsed":3320,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"f0b8f08d-cbe7-4fc6-82ef-3bf6a1291164"},"source":["import torch\n","import numpy\n","import matplotlib.pyplot as plt\n","import datasets\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import numpy as np\n","import argparse\n","import pdb\n","\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import LineByLineTextDataset\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from transformers import RobertaConfig, RobertaModelWithHeads\n","\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U8qnQG9_hQdq","executionInfo":{"status":"ok","timestamp":1619967403641,"user_tz":240,"elapsed":572,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}}},"source":["args_dict = {\n","    \"train_dataset\": \"/content/drive/MyDrive/OMSCS_DL/CITATION/data/train.jsonl\",\n","    \"val_dataset\": \"/content/drive/MyDrive/OMSCS_DL/CITATION/data/dev.jsonl\",\n","    \"test_dataset\": \"/content/drive/MyDrive/OMSCS_DL/CITATION/data/test.jsonl\",\n","    \"model\": \"roberta-base\",\n","    \"output_dir\": \"/content/drive/MyDrive/OMSCS_DL/CITATION/output/mlm_tune/mlm_classify/\",\n","    \"adapter_name\": \"mlm_CITAION_pretrain\"\n","}"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"izmKUeYQiaC9","executionInfo":{"status":"ok","timestamp":1619967404970,"user_tz":240,"elapsed":662,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}}},"source":["# define accuracy metrics\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEtVyWuJicSF","executionInfo":{"status":"ok","timestamp":1619967408494,"user_tz":240,"elapsed":3008,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"d657fbfd-8aed-4b63-9447-8fd4404dc724"},"source":["# loading the data\n","train_data = datasets.load_dataset(\"json\",data_files=args_dict[\"train_dataset\"])[\"train\"]\n","#not a typo: load_dataset loads this as training data\n","val_data = datasets.load_dataset(\n","                \"json\", data_files=args_dict[\"val_dataset\"])[\"train\"] \n","test_data = datasets.load_dataset(\n","                \"json\", data_files=args_dict[\"test_dataset\"])[\"train\"] "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using custom data configuration default-e6a7c44083f2214b\n","Reusing dataset json (/root/.cache/huggingface/datasets/json/default-e6a7c44083f2214b/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n","Using custom data configuration default-bc5e6cf9187051d7\n","Reusing dataset json (/root/.cache/huggingface/datasets/json/default-bc5e6cf9187051d7/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n","Using custom data configuration default-a7212a96152b8614\n","Reusing dataset json (/root/.cache/huggingface/datasets/json/default-a7212a96152b8614/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pRZ1jcXLigPG","executionInfo":{"status":"ok","timestamp":1619967413572,"user_tz":240,"elapsed":4193,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}}},"source":["#assign an integer key for each label\n","label_keys = {}\n","num_labels = 0\n","for label in train_data['label']:\n","    if label not in label_keys:\n","        label_keys[label] = num_labels\n","        num_labels += 1\n","\n","for label in test_data['label']:\n","    if label not in label_keys:\n","        label_keys[label] = num_labels\n","        num_labels += 1\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["2db29a06f2584eccaaa5fe82202bffc3","4669242fd69a491f929144a27bacb9e0","15780094a0c54ee88445788aebe5b4a3","1afb768a4dbf482985e4c0e87307b7b8","7f5f68fc3ad940d9a6b7e2a94c072964","a9443c5cdceb49aaa6702bc1fa9154b2","b5486e0793aa4a0a99fba1fed663d4c2","420ad8bc5ae14d30ac6628b0143697f5","27a06b520a9b497b9693d13aadb50029","e6ad94fd922344f288b3bccf138f723f","d701856b2cc44c99ad9f06a7fbeb5154","3b1d2be5957c423fb3b8d388518e8dd3","42417cc2480e4548be78139b890d5761","d336da6fe4484458a923fb39f1b561a3","e993e25a0338468d8e4eb4e287756e5b","5bf5c7403ca64e23adbee4fe01377f08","022937e7bea548e49fb6addbe447eca7","b2a747c438e445fb87855eab6e1252b9","fb3fed9a9ba14c1da6a92f7ce3a943e0","0176066aed6f454d961596203d854909","95f3863565b6481ab1910e09f033ee7c","de41e8f9d5d04212b124667588699b0d","180e839d0af7442bb34e03c544f80104","ebbd8444f50e4925b74cde16065a633c"]},"id":"HuxTdup6iioe","executionInfo":{"status":"ok","timestamp":1619967420844,"user_tz":240,"elapsed":4680,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"3ef9c643-d96f-450b-e270-cb6e47628dfb"},"source":["def tokenization(batched_text):\n","    tokenized_batch = tokenizer(batched_text['text'], padding=True, truncation=True)\n","    tokenized_batch[\"label\"] = [label_keys[label] for label in batched_text[\"label\"]]\n","    return tokenized_batch\n","\n","train_data = train_data.map(tokenization, batched=True, batch_size=len(train_data))\n","val_data = val_data.map(tokenization, batched=True, batch_size=len(val_data))\n","test_data = test_data.map(tokenization, batched=True, batch_size=len(test_data))"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2db29a06f2584eccaaa5fe82202bffc3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27a06b520a9b497b9693d13aadb50029","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"022937e7bea548e49fb6addbe447eca7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8okuQRCimvm","executionInfo":{"status":"ok","timestamp":1619967422333,"user_tz":240,"elapsed":490,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}}},"source":["train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n","val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n","test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7MdW2fRWipAm","executionInfo":{"status":"ok","timestamp":1619971754868,"user_tz":240,"elapsed":3544880,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"ff55a51c-40d9-4215-e148-6c5e802e29cc"},"source":["f1_scores = []\n","for seed in range(1,4):\n","  #create model\n","  model_prev = RobertaForSequenceClassification.from_pretrained(\n","        \"roberta-base\", hidden_dropout_prob=0.1, num_labels=num_labels)\n","  model_prev_configuration = model_prev.config\n","\n","  model = RobertaModelWithHeads.from_pretrained(\n","    \"roberta-base\",\n","    config=model_prev_configuration,\n","  )\n","\n","  model.load_adapter(\"/content/drive/MyDrive/OMSCS_DL/CITATION/output/mlm_tune/mlm_pretrain/mlm_CITAION_pretrain\")\n","\n","  model.add_classification_head(\n","  'mlm_CITAION_pretrain',\n","  num_labels=num_labels,\n","  )\n","  # Activate the adapter\n","  model.train_adapter('mlm_CITAION_pretrain')\n","\n","  #adapter_name = args_dict[\"adapter_name\"] + str(seed)\n","    \n","\n","\n","  training_args = TrainingArguments(\n","      num_train_epochs=25,\n","      per_device_train_batch_size=16,\n","      per_device_eval_batch_size=16,\n","      learning_rate=5e-5,\n","      lr_scheduler_type=\"constant\",\n","      fp16=True,\n","      eval_accumulation_steps=20,\n","      save_strategy=\"no\",\n","      save_steps=5000,\n","      save_total_limit=1,\n","      load_best_model_at_end=False,\n","      metric_for_best_model=\"f1\",\n","      evaluation_strategy=\"epoch\",\n","      output_dir=args_dict[\"output_dir\"],\n","      overwrite_output_dir=True,\n","      # The next line is important to ensure the dataset labels are properly passed to the model\n","      remove_unused_columns=False,\n","      seed = seed\n","  )\n","\n","  trainer = Trainer(\n","      model = model,\n","      args=training_args,\n","      compute_metrics=compute_metrics,\n","      train_dataset=train_data,\n","      eval_dataset=val_data\n","  )\n","\n","\n","  trainer.train()\n","\n","  metrics = trainer.evaluate(test_data)\n","  print(\"Metrics for seed {}: {}\".format(seed, metrics))\n","\n","  f1_scores.append(metrics[\"eval_f1\"])\n","\n","  model.save_adapter(args_dict[\"output_dir\"] + str(seed) + \"/\", \"mlm_CITAION_pretrain\", with_head=True)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='2650' max='2650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2650/2650 19:25, Epoch 25/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.304145</td>\n","      <td>0.517544</td>\n","      <td>0.113680</td>\n","      <td>0.086257</td>\n","      <td>0.166667</td>\n","      <td>0.811900</td>\n","      <td>140.408000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.019932</td>\n","      <td>0.657895</td>\n","      <td>0.263030</td>\n","      <td>0.234830</td>\n","      <td>0.303874</td>\n","      <td>0.790400</td>\n","      <td>144.229000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.878266</td>\n","      <td>0.701754</td>\n","      <td>0.323551</td>\n","      <td>0.390366</td>\n","      <td>0.348668</td>\n","      <td>0.791800</td>\n","      <td>143.981000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.821370</td>\n","      <td>0.710526</td>\n","      <td>0.344136</td>\n","      <td>0.352937</td>\n","      <td>0.361716</td>\n","      <td>0.793000</td>\n","      <td>143.760000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.118500</td>\n","      <td>0.758815</td>\n","      <td>0.701754</td>\n","      <td>0.409860</td>\n","      <td>0.521675</td>\n","      <td>0.397733</td>\n","      <td>0.800700</td>\n","      <td>142.381000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.118500</td>\n","      <td>0.769986</td>\n","      <td>0.736842</td>\n","      <td>0.506635</td>\n","      <td>0.523902</td>\n","      <td>0.507163</td>\n","      <td>0.794500</td>\n","      <td>143.489000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.118500</td>\n","      <td>0.730682</td>\n","      <td>0.745614</td>\n","      <td>0.509438</td>\n","      <td>0.543056</td>\n","      <td>0.509988</td>\n","      <td>0.775600</td>\n","      <td>146.977000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.118500</td>\n","      <td>0.722238</td>\n","      <td>0.728070</td>\n","      <td>0.573859</td>\n","      <td>0.701621</td>\n","      <td>0.534685</td>\n","      <td>0.787200</td>\n","      <td>144.815000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.118500</td>\n","      <td>0.759604</td>\n","      <td>0.745614</td>\n","      <td>0.636863</td>\n","      <td>0.751981</td>\n","      <td>0.599623</td>\n","      <td>0.772400</td>\n","      <td>147.595000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.744900</td>\n","      <td>0.693613</td>\n","      <td>0.736842</td>\n","      <td>0.627238</td>\n","      <td>0.739048</td>\n","      <td>0.586575</td>\n","      <td>0.815900</td>\n","      <td>139.727000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.744900</td>\n","      <td>0.698094</td>\n","      <td>0.745614</td>\n","      <td>0.657086</td>\n","      <td>0.700150</td>\n","      <td>0.635243</td>\n","      <td>0.791800</td>\n","      <td>143.970000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.744900</td>\n","      <td>0.767747</td>\n","      <td>0.780702</td>\n","      <td>0.660713</td>\n","      <td>0.814815</td>\n","      <td>0.610923</td>\n","      <td>0.782600</td>\n","      <td>145.665000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.744900</td>\n","      <td>0.696263</td>\n","      <td>0.780702</td>\n","      <td>0.652328</td>\n","      <td>0.692593</td>\n","      <td>0.646543</td>\n","      <td>0.791100</td>\n","      <td>144.111000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.744900</td>\n","      <td>0.784844</td>\n","      <td>0.780702</td>\n","      <td>0.677418</td>\n","      <td>0.758251</td>\n","      <td>0.666828</td>\n","      <td>0.778300</td>\n","      <td>146.470000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.574800</td>\n","      <td>0.723877</td>\n","      <td>0.789474</td>\n","      <td>0.691585</td>\n","      <td>0.816713</td>\n","      <td>0.644256</td>\n","      <td>0.809700</td>\n","      <td>140.792000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.574800</td>\n","      <td>0.710875</td>\n","      <td>0.780702</td>\n","      <td>0.679858</td>\n","      <td>0.736329</td>\n","      <td>0.677051</td>\n","      <td>0.774000</td>\n","      <td>147.283000</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.574800</td>\n","      <td>0.675848</td>\n","      <td>0.780702</td>\n","      <td>0.679080</td>\n","      <td>0.762204</td>\n","      <td>0.646543</td>\n","      <td>0.794000</td>\n","      <td>143.568000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.574800</td>\n","      <td>0.754429</td>\n","      <td>0.771930</td>\n","      <td>0.638349</td>\n","      <td>0.705473</td>\n","      <td>0.633495</td>\n","      <td>0.784800</td>\n","      <td>145.253000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.450900</td>\n","      <td>0.787196</td>\n","      <td>0.763158</td>\n","      <td>0.659737</td>\n","      <td>0.728353</td>\n","      <td>0.666290</td>\n","      <td>0.798800</td>\n","      <td>142.707000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.450900</td>\n","      <td>0.807558</td>\n","      <td>0.754386</td>\n","      <td>0.659532</td>\n","      <td>0.700634</td>\n","      <td>0.668577</td>\n","      <td>0.790400</td>\n","      <td>144.224000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.450900</td>\n","      <td>0.789422</td>\n","      <td>0.798246</td>\n","      <td>0.681346</td>\n","      <td>0.745238</td>\n","      <td>0.682701</td>\n","      <td>0.776500</td>\n","      <td>146.821000</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.450900</td>\n","      <td>0.831938</td>\n","      <td>0.763158</td>\n","      <td>0.669881</td>\n","      <td>0.719008</td>\n","      <td>0.671402</td>\n","      <td>0.780000</td>\n","      <td>146.150000</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.450900</td>\n","      <td>0.804813</td>\n","      <td>0.789474</td>\n","      <td>0.684182</td>\n","      <td>0.721155</td>\n","      <td>0.690100</td>\n","      <td>0.791200</td>\n","      <td>144.076000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.344300</td>\n","      <td>0.808631</td>\n","      <td>0.789474</td>\n","      <td>0.676787</td>\n","      <td>0.731218</td>\n","      <td>0.679876</td>\n","      <td>0.787400</td>\n","      <td>144.772000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.344300</td>\n","      <td>0.782065</td>\n","      <td>0.798246</td>\n","      <td>0.659227</td>\n","      <td>0.705087</td>\n","      <td>0.657304</td>\n","      <td>0.788600</td>\n","      <td>144.566000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Metrics for seed 1: {'eval_loss': 0.9997644424438477, 'eval_accuracy': 0.7769784172661871, 'eval_f1': 0.695157032077562, 'eval_precision': 0.7474251238957121, 'eval_recall': 0.6767915183408141, 'eval_runtime': 0.8936, 'eval_samples_per_second': 155.548, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 88568320}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='2650' max='2650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2650/2650 19:24, Epoch 25/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.332864</td>\n","      <td>0.517544</td>\n","      <td>0.113680</td>\n","      <td>0.086257</td>\n","      <td>0.166667</td>\n","      <td>0.777300</td>\n","      <td>146.661000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.072129</td>\n","      <td>0.631579</td>\n","      <td>0.246134</td>\n","      <td>0.226064</td>\n","      <td>0.280065</td>\n","      <td>0.781800</td>\n","      <td>145.812000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.877110</td>\n","      <td>0.736842</td>\n","      <td>0.360817</td>\n","      <td>0.355064</td>\n","      <td>0.380414</td>\n","      <td>0.801100</td>\n","      <td>142.300000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.849532</td>\n","      <td>0.719298</td>\n","      <td>0.349750</td>\n","      <td>0.374158</td>\n","      <td>0.374765</td>\n","      <td>0.785400</td>\n","      <td>145.150000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.138100</td>\n","      <td>0.794478</td>\n","      <td>0.692982</td>\n","      <td>0.398040</td>\n","      <td>0.496895</td>\n","      <td>0.394909</td>\n","      <td>0.782400</td>\n","      <td>145.711000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.138100</td>\n","      <td>0.735762</td>\n","      <td>0.710526</td>\n","      <td>0.418427</td>\n","      <td>0.511402</td>\n","      <td>0.410782</td>\n","      <td>0.785500</td>\n","      <td>145.128000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.138100</td>\n","      <td>0.740730</td>\n","      <td>0.736842</td>\n","      <td>0.502503</td>\n","      <td>0.545628</td>\n","      <td>0.486716</td>\n","      <td>0.783800</td>\n","      <td>145.443000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.138100</td>\n","      <td>0.717286</td>\n","      <td>0.719298</td>\n","      <td>0.518110</td>\n","      <td>0.504307</td>\n","      <td>0.540355</td>\n","      <td>0.813600</td>\n","      <td>140.111000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.138100</td>\n","      <td>0.715661</td>\n","      <td>0.728070</td>\n","      <td>0.501670</td>\n","      <td>0.519097</td>\n","      <td>0.504338</td>\n","      <td>0.794100</td>\n","      <td>143.565000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.740300</td>\n","      <td>0.787953</td>\n","      <td>0.719298</td>\n","      <td>0.499221</td>\n","      <td>0.501616</td>\n","      <td>0.535243</td>\n","      <td>0.801700</td>\n","      <td>142.191000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.740300</td>\n","      <td>0.861581</td>\n","      <td>0.719298</td>\n","      <td>0.534271</td>\n","      <td>0.529991</td>\n","      <td>0.579197</td>\n","      <td>0.802000</td>\n","      <td>142.151000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.740300</td>\n","      <td>0.739033</td>\n","      <td>0.763158</td>\n","      <td>0.632965</td>\n","      <td>0.702083</td>\n","      <td>0.610385</td>\n","      <td>0.825600</td>\n","      <td>138.082000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.740300</td>\n","      <td>0.731277</td>\n","      <td>0.789474</td>\n","      <td>0.606487</td>\n","      <td>0.675680</td>\n","      <td>0.580017</td>\n","      <td>0.809400</td>\n","      <td>140.839000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.740300</td>\n","      <td>0.696603</td>\n","      <td>0.780702</td>\n","      <td>0.682455</td>\n","      <td>0.712432</td>\n","      <td>0.675161</td>\n","      <td>0.806200</td>\n","      <td>141.404000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.562300</td>\n","      <td>0.734312</td>\n","      <td>0.789474</td>\n","      <td>0.667644</td>\n","      <td>0.751740</td>\n","      <td>0.657701</td>\n","      <td>0.806400</td>\n","      <td>141.368000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.562300</td>\n","      <td>0.826362</td>\n","      <td>0.745614</td>\n","      <td>0.620170</td>\n","      <td>0.669092</td>\n","      <td>0.609847</td>\n","      <td>0.775500</td>\n","      <td>147.000000</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.562300</td>\n","      <td>0.750302</td>\n","      <td>0.754386</td>\n","      <td>0.599576</td>\n","      <td>0.598033</td>\n","      <td>0.621005</td>\n","      <td>0.789400</td>\n","      <td>144.420000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.562300</td>\n","      <td>0.709499</td>\n","      <td>0.789474</td>\n","      <td>0.679149</td>\n","      <td>0.710877</td>\n","      <td>0.679876</td>\n","      <td>0.789200</td>\n","      <td>144.442000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.439000</td>\n","      <td>0.864636</td>\n","      <td>0.754386</td>\n","      <td>0.664335</td>\n","      <td>0.678382</td>\n","      <td>0.678800</td>\n","      <td>0.787900</td>\n","      <td>144.685000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.439000</td>\n","      <td>0.816165</td>\n","      <td>0.789474</td>\n","      <td>0.712812</td>\n","      <td>0.713123</td>\n","      <td>0.728941</td>\n","      <td>0.809900</td>\n","      <td>140.757000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.439000</td>\n","      <td>0.937225</td>\n","      <td>0.745614</td>\n","      <td>0.627828</td>\n","      <td>0.634698</td>\n","      <td>0.653800</td>\n","      <td>0.805400</td>\n","      <td>141.537000</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.439000</td>\n","      <td>0.859995</td>\n","      <td>0.798246</td>\n","      <td>0.693423</td>\n","      <td>0.832043</td>\n","      <td>0.657304</td>\n","      <td>0.771500</td>\n","      <td>147.764000</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.439000</td>\n","      <td>0.848214</td>\n","      <td>0.763158</td>\n","      <td>0.677788</td>\n","      <td>0.710507</td>\n","      <td>0.671402</td>\n","      <td>0.785700</td>\n","      <td>145.092000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.331000</td>\n","      <td>0.917342</td>\n","      <td>0.789474</td>\n","      <td>0.702054</td>\n","      <td>0.822375</td>\n","      <td>0.690100</td>\n","      <td>0.788800</td>\n","      <td>144.522000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.331000</td>\n","      <td>0.933245</td>\n","      <td>0.763158</td>\n","      <td>0.682933</td>\n","      <td>0.716421</td>\n","      <td>0.686737</td>\n","      <td>0.798300</td>\n","      <td>142.811000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Metrics for seed 2: {'eval_loss': 1.0181424617767334, 'eval_accuracy': 0.7841726618705036, 'eval_f1': 0.6970067101272774, 'eval_precision': 0.7003968253968252, 'eval_recall': 0.700735180312645, 'eval_runtime': 0.8906, 'eval_samples_per_second': 156.066, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 88568320}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='2650' max='2650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2650/2650 19:25, Epoch 25/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.319257</td>\n","      <td>0.517544</td>\n","      <td>0.113680</td>\n","      <td>0.086257</td>\n","      <td>0.166667</td>\n","      <td>0.791400</td>\n","      <td>144.049000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.254582</td>\n","      <td>0.517544</td>\n","      <td>0.113680</td>\n","      <td>0.086257</td>\n","      <td>0.166667</td>\n","      <td>0.775900</td>\n","      <td>146.934000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.016486</td>\n","      <td>0.675439</td>\n","      <td>0.303124</td>\n","      <td>0.378629</td>\n","      <td>0.340194</td>\n","      <td>0.793800</td>\n","      <td>143.612000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.925127</td>\n","      <td>0.701754</td>\n","      <td>0.341859</td>\n","      <td>0.333150</td>\n","      <td>0.374227</td>\n","      <td>0.780700</td>\n","      <td>146.019000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.196400</td>\n","      <td>0.847637</td>\n","      <td>0.675439</td>\n","      <td>0.330111</td>\n","      <td>0.313825</td>\n","      <td>0.360640</td>\n","      <td>0.797100</td>\n","      <td>143.010000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.196400</td>\n","      <td>0.801602</td>\n","      <td>0.675439</td>\n","      <td>0.385384</td>\n","      <td>0.512250</td>\n","      <td>0.379036</td>\n","      <td>0.783100</td>\n","      <td>145.574000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.196400</td>\n","      <td>0.776037</td>\n","      <td>0.692982</td>\n","      <td>0.402785</td>\n","      <td>0.492835</td>\n","      <td>0.405132</td>\n","      <td>0.786100</td>\n","      <td>145.012000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.196400</td>\n","      <td>0.734516</td>\n","      <td>0.701754</td>\n","      <td>0.464265</td>\n","      <td>0.505708</td>\n","      <td>0.457022</td>\n","      <td>0.791300</td>\n","      <td>144.075000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.196400</td>\n","      <td>0.703647</td>\n","      <td>0.745614</td>\n","      <td>0.513513</td>\n","      <td>0.495455</td>\n","      <td>0.553941</td>\n","      <td>0.799600</td>\n","      <td>142.565000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.786000</td>\n","      <td>0.722980</td>\n","      <td>0.719298</td>\n","      <td>0.477032</td>\n","      <td>0.460640</td>\n","      <td>0.501513</td>\n","      <td>0.767600</td>\n","      <td>148.519000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.786000</td>\n","      <td>0.857384</td>\n","      <td>0.728070</td>\n","      <td>0.479364</td>\n","      <td>0.494444</td>\n","      <td>0.527845</td>\n","      <td>0.776500</td>\n","      <td>146.820000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.786000</td>\n","      <td>0.738652</td>\n","      <td>0.745614</td>\n","      <td>0.633772</td>\n","      <td>0.749949</td>\n","      <td>0.584288</td>\n","      <td>0.788100</td>\n","      <td>144.650000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.786000</td>\n","      <td>0.696440</td>\n","      <td>0.719298</td>\n","      <td>0.553476</td>\n","      <td>0.552361</td>\n","      <td>0.563862</td>\n","      <td>0.838200</td>\n","      <td>136.004000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.786000</td>\n","      <td>0.766391</td>\n","      <td>0.736842</td>\n","      <td>0.633711</td>\n","      <td>0.655159</td>\n","      <td>0.654197</td>\n","      <td>0.823800</td>\n","      <td>138.386000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.592400</td>\n","      <td>0.853933</td>\n","      <td>0.701754</td>\n","      <td>0.528263</td>\n","      <td>0.529309</td>\n","      <td>0.563324</td>\n","      <td>0.801400</td>\n","      <td>142.248000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.592400</td>\n","      <td>0.729712</td>\n","      <td>0.754386</td>\n","      <td>0.660771</td>\n","      <td>0.644200</td>\n","      <td>0.710640</td>\n","      <td>0.811900</td>\n","      <td>140.419000</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.592400</td>\n","      <td>0.742458</td>\n","      <td>0.719298</td>\n","      <td>0.634869</td>\n","      <td>0.607903</td>\n","      <td>0.689118</td>\n","      <td>0.794600</td>\n","      <td>143.470000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.592400</td>\n","      <td>0.776071</td>\n","      <td>0.745614</td>\n","      <td>0.616291</td>\n","      <td>0.707756</td>\n","      <td>0.623292</td>\n","      <td>0.795200</td>\n","      <td>143.364000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.456600</td>\n","      <td>0.851034</td>\n","      <td>0.754386</td>\n","      <td>0.669534</td>\n","      <td>0.676190</td>\n","      <td>0.715752</td>\n","      <td>0.809900</td>\n","      <td>140.759000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.456600</td>\n","      <td>0.812168</td>\n","      <td>0.780702</td>\n","      <td>0.697632</td>\n","      <td>0.769460</td>\n","      <td>0.685385</td>\n","      <td>0.791700</td>\n","      <td>143.987000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.456600</td>\n","      <td>0.813691</td>\n","      <td>0.763158</td>\n","      <td>0.679107</td>\n","      <td>0.702298</td>\n","      <td>0.705132</td>\n","      <td>0.799300</td>\n","      <td>142.620000</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.456600</td>\n","      <td>0.807515</td>\n","      <td>0.763158</td>\n","      <td>0.711222</td>\n","      <td>0.788768</td>\n","      <td>0.710243</td>\n","      <td>0.814400</td>\n","      <td>139.980000</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.456600</td>\n","      <td>0.950984</td>\n","      <td>0.710526</td>\n","      <td>0.638095</td>\n","      <td>0.615133</td>\n","      <td>0.716963</td>\n","      <td>0.783300</td>\n","      <td>145.544000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.347400</td>\n","      <td>0.935347</td>\n","      <td>0.763158</td>\n","      <td>0.687730</td>\n","      <td>0.701170</td>\n","      <td>0.720467</td>\n","      <td>0.790600</td>\n","      <td>144.186000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.347400</td>\n","      <td>0.892543</td>\n","      <td>0.798246</td>\n","      <td>0.729252</td>\n","      <td>0.785727</td>\n","      <td>0.734988</td>\n","      <td>0.795600</td>\n","      <td>143.284000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Metrics for seed 3: {'eval_loss': 1.0232982635498047, 'eval_accuracy': 0.762589928057554, 'eval_f1': 0.6741103961600855, 'eval_precision': 0.6823629586787482, 'eval_recall': 0.6816204921838724, 'eval_runtime': 0.8717, 'eval_samples_per_second': 159.465, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 88568320}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyI6smQqk8e1","executionInfo":{"status":"ok","timestamp":1619971793236,"user_tz":240,"elapsed":722,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"b0718c82-9678-4c02-a135-cb87909b8dd5"},"source":["f1_scores"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.695157032077562, 0.6970067101272774, 0.6741103961600855]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJHbJZly13HP","executionInfo":{"status":"ok","timestamp":1619971800639,"user_tz":240,"elapsed":924,"user":{"displayName":"Vincent Zhou","photoUrl":"","userId":"05308563869087671334"}},"outputId":"b7d61f3e-fab7-428f-a62c-7dfbe8c2d6c0"},"source":["print(\"Avg f1 score: {}, stdev: {}\".format(np.mean(f1_scores), np.std(f1_scores)))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Avg f1 score: 0.6887580461216416, stdev: 0.010384943083429535\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GzIInlYO1_pl"},"source":[""],"execution_count":null,"outputs":[]}]}